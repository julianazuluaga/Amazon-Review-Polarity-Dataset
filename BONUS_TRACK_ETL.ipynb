{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "loYDtd5K3TB_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4f85f19-4650-4a54-ec16-b8b7555df6f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: vaderSentiment in /usr/local/lib/python3.10/dist-packages (3.3.2)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (7.4.4)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from vaderSentiment) (2.32.3)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytest) (24.1)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest) (1.5.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest) (1.2.2)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest) (2.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2024.8.30)\n",
            "Requirement already satisfied: vaderSentiment in /usr/local/lib/python3.10/dist-packages (3.3.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from vaderSentiment) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2024.8.30)\n",
            "Requirement already satisfied: catboost in /usr/local/lib/python3.10/dist-packages (1.2.7)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.26.4)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from catboost) (1.13.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (3.2.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost) (9.0.0)\n"
          ]
        }
      ],
      "source": [
        "# Instalar dependencias\n",
        "!pip install pandas nltk scikit-learn vaderSentiment pytest\n",
        "!pip install vaderSentiment\n",
        "!pip install catboost"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Construir el Pipeline de Datos**\n",
        "Este enfoque construye un pipeline de procesamiento, verifica la calidad de los datos con controles clave y usa pruebas de unidad para asegurar que cada función del pipeline funcione correctamente. Esta estructura garantiza que el pipeline de procesamiento de datos esté bien diseñado, probado y listo para su uso en modelos de análisis de sentimiento.\n",
        "\n",
        "El pipeline procesará los datos desde su estado original hasta el estado final, listos para su uso en un modelo de clasificación de sentimiento.\n"
      ],
      "metadata": {
        "id": "cE85WTb-3TsU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# my_pipeline.py\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "import os\n",
        "import kagglehub\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "import nltk\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "\n",
        "# Inicializar componentes necesarios\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "vectorizer = TfidfVectorizer(max_features=1000)\n",
        "\n",
        "# Descargar y cargar datos de Kaggle con una muestra limitada\n",
        "def load_data():\n",
        "    path = kagglehub.dataset_download(\"kritanjalijain/amazon-reviews\")\n",
        "    train_file = os.path.join(path, 'train.csv')\n",
        "    test_file = os.path.join(path, 'test.csv')\n",
        "\n",
        "    columns = ['polaridad', 'titulo', 'texto']\n",
        "    df_train = pd.read_csv(train_file, names=columns).head(5000)\n",
        "    df_test = pd.read_csv(test_file, names=columns).head(1000)\n",
        "\n",
        "    df_train = concat_columns(df_train, 'texto', 'titulo', 'texto')\n",
        "    df_test = concat_columns(df_test, 'texto', 'titulo', 'texto')\n",
        "    df_train['polaridad'] = df_train['polaridad'].map({1:0, 2:1})\n",
        "    df_test['polaridad'] = df_test['polaridad'].map({1:0, 2:1})\n",
        "\n",
        "    return df_train, df_test\n",
        "\n",
        "# Funciones auxiliares\n",
        "def concat_columns(df, col1, col2, new_col):\n",
        "    df[new_col] = df[col1].apply(str) + ' ' + df[col2].apply(str)\n",
        "    df.drop(col2, axis=1, inplace=True)\n",
        "    return df\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()  # Convertir a minúsculas\n",
        "    text = re.sub(r'http\\S+|www\\S+', '', text)  # Eliminar URLs (http, https y www)\n",
        "    text = re.sub(r'@\\w+', '', text)  # Eliminar menciones (@usuario)\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))  # Eliminar signos de puntuación\n",
        "    return text.strip()  # Eliminar espacios adicionales al inicio y final\n",
        "\n",
        "def tokenize_and_stem(text):\n",
        "    tokens = [stemmer.stem(word) for word in word_tokenize(text) if word not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "def preprocess_data(df):\n",
        "    df['texto_limpio'] = df['texto'].apply(clean_text)\n",
        "    df['tokens'] = df['texto_limpio'].apply(tokenize_and_stem)\n",
        "    return df\n",
        "\n",
        "def sentiment_analysis(df):\n",
        "    df['compound'] = df['texto_limpio'].apply(lambda x: analyzer.polarity_scores(x)['compound'])\n",
        "    df['sentiment'] = df['compound'].apply(lambda score: 'Positivo' if score > 0.05 else 'Negativo' if score < -0.05 else 'Neutral')\n",
        "    return df\n",
        "\n",
        "# Función para vectorizar el texto\n",
        "def vectorize_text(df_train, df_test):\n",
        "    X_train = vectorizer.fit_transform(df_train['texto_limpio'])\n",
        "    X_test = vectorizer.transform(df_test['texto_limpio'])\n",
        "    return X_train, X_test\n",
        "\n",
        "\n",
        "# Función para configurar, entrenar y evaluar el modelo\n",
        "def train_model(df_train, df_test):\n",
        "    # Parámetros del preprocesamiento\n",
        "    max_words = 10000\n",
        "    max_len = 200\n",
        "\n",
        "    # Tokenización y preprocesamiento de texto\n",
        "    tokenizer = Tokenizer(num_words=max_words)\n",
        "    tokenizer.fit_on_texts(df_train['texto_limpio'])\n",
        "\n",
        "    X_train = pad_sequences(tokenizer.texts_to_sequences(df_train['texto_limpio']), maxlen=max_len)\n",
        "    X_test = pad_sequences(tokenizer.texts_to_sequences(df_test['texto_limpio']), maxlen=max_len)\n",
        "\n",
        "    y_train = df_train['polaridad'].values\n",
        "    y_test = df_test['polaridad'].values\n",
        "\n",
        "    # Configuración del modelo\n",
        "    embedding_dim = 16\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len),\n",
        "        GlobalAveragePooling1D(),\n",
        "        Dense(16, activation='relu'),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    # Compilar el modelo\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Configurar EarlyStopping para evitar sobreajuste\n",
        "    early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "    # Entrenar el modelo\n",
        "    model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2, callbacks=[early_stop])\n",
        "\n",
        "    # Evaluar en el conjunto de prueba\n",
        "    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Test Loss: {loss:.4f}\")\n",
        "\n",
        "    # Obtener predicciones en el conjunto de prueba\n",
        "    y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
        "\n",
        "    # Calcular métricas adicionales\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "# Ejecutar el pipeline completo\n",
        "if __name__ == \"__main__\":\n",
        "    df_train, df_test = load_data()\n",
        "    df_train = preprocess_data(df_train)\n",
        "    df_test = preprocess_data(df_test)\n",
        "    train_model(df_train, df_test)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0SH4BJOzeOX",
        "outputId": "26a80a7b-1061-45d3-d32b-b252d38550ba"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.5160 - loss: 0.6924 - val_accuracy: 0.5710 - val_loss: 0.6858\n",
            "Epoch 2/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5282 - loss: 0.6857 - val_accuracy: 0.6330 - val_loss: 0.6653\n",
            "Epoch 3/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5896 - loss: 0.6527 - val_accuracy: 0.7040 - val_loss: 0.6238\n",
            "Epoch 4/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7686 - loss: 0.5628 - val_accuracy: 0.8070 - val_loss: 0.5177\n",
            "Epoch 5/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8229 - loss: 0.4616 - val_accuracy: 0.8450 - val_loss: 0.4441\n",
            "Epoch 6/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8800 - loss: 0.3708 - val_accuracy: 0.8360 - val_loss: 0.4033\n",
            "Epoch 7/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9081 - loss: 0.3081 - val_accuracy: 0.8570 - val_loss: 0.3709\n",
            "Epoch 8/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9010 - loss: 0.2869 - val_accuracy: 0.8680 - val_loss: 0.3543\n",
            "Epoch 9/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9152 - loss: 0.2532 - val_accuracy: 0.8570 - val_loss: 0.3573\n",
            "Epoch 10/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9184 - loss: 0.2305 - val_accuracy: 0.8690 - val_loss: 0.3357\n",
            "Test Accuracy: 0.8490\n",
            "Test Loss: 0.3603\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
            "Precision: 0.8589\n",
            "Recall: 0.8367\n",
            "F1 Score: 0.8476\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Pruebas de Unidad para Validar el Pipeline**\n",
        "A continuación, se define un conjunto de pruebas de unidad para validar las transformaciones de texto y las salidas de cada etapa del pipeline.\n",
        "\n"
      ],
      "metadata": {
        "id": "CYo7irb3GeJO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pruebas unitarias\n",
        "\n",
        "def test_load_data():\n",
        "    df_train, df_test = load_data()\n",
        "    assert not df_train.empty, \"El DataFrame de entrenamiento está vacío.\"\n",
        "    assert not df_test.empty, \"El DataFrame de prueba está vacío.\"\n",
        "\n",
        "def test_clean_text():\n",
        "    raw_text = \"This is a test! Check out https://example.com @user\"\n",
        "    expected = \"this is a test check out\"\n",
        "    result = clean_text(raw_text)\n",
        "    assert result == expected, f\"La función clean_text no limpia correctamente el texto. Resultado: {result}\"\n",
        "\n",
        "def test_tokenize_and_stem():\n",
        "    text = \"this is a simple text example\"\n",
        "    expected_tokens = [\"simpl\", \"text\", \"exampl\"]\n",
        "    assert tokenize_and_stem(text) == expected_tokens, \"La función tokenize_and_stem no funciona correctamente.\"\n",
        "\n",
        "def test_preprocess_data():\n",
        "    df_train, _ = load_data()\n",
        "    df_processed = preprocess_data(df_train)\n",
        "    assert 'texto_limpio' in df_processed.columns, \"La columna 'texto_limpio' falta en el DataFrame procesado.\"\n",
        "    assert 'tokens' in df_processed.columns, \"La columna 'tokens' falta en el DataFrame procesado.\"\n",
        "\n",
        "def test_sentiment_analysis():\n",
        "    data = {'texto_limpio': [\"I love this!\", \"I hate this!\", \"It's ok.\"]}\n",
        "    df = pd.DataFrame(data)\n",
        "    df = sentiment_analysis(df)\n",
        "    assert set(df['sentiment']) == {\"Positivo\", \"Negativo\", \"Neutral\"}, \"La función sentiment_analysis no clasifica correctamente los sentimientos.\"\n",
        "\n",
        "# Ejecutar las pruebas\n",
        "test_load_data()\n",
        "test_clean_text()\n",
        "test_tokenize_and_stem()\n",
        "test_preprocess_data()\n",
        "test_sentiment_analysis()\n",
        "\n",
        "print(\"Todas las pruebas pasaron correctamente.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2mUuvD80cxp",
        "outputId": "eeefa818-88d0-4dad-ca4a-c9cf949e83eb"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Todas las pruebas pasaron correctamente.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejecutar Controles de Calidad**\n",
        "Los controles de calidad aseguran que los datos procesados cumplan con los estándares esperados.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RIM3kA1pz-U8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para controles de calidad en los datos procesados\n",
        "def quality_checks(df):\n",
        "    # 1. Verificar que no haya valores nulos\n",
        "    assert df.isnull().sum().sum() == 0, \"Hay valores nulos en el dataframe procesado.\"\n",
        "\n",
        "    # 2. Verificar que polaridad tenga solo valores binarios (0 y 1)\n",
        "    assert set(df['polaridad'].unique()).issubset({0, 1}), \"La columna 'polaridad' contiene valores no binarios.\"\n",
        "\n",
        "    # 3. Comprobar columnas necesarias en el dataframe\n",
        "    required_columns = ['texto', 'polaridad', 'texto_limpio', 'tokens', 'compound', 'sentiment']\n",
        "    for col in required_columns:\n",
        "        assert col in df.columns, f\"La columna {col} falta en el dataframe.\"\n",
        "\n",
        "    print(\"Todos los controles de calidad se aprobaron.\")\n",
        "\n",
        "# Pipeline completo de datos\n",
        "def data_pipeline():\n",
        "    # Cargar datos de Kaggle y aplicar muestra\n",
        "    df_train, df_test = load_data()\n",
        "\n",
        "    # Preprocesamiento y análisis de sentimiento\n",
        "    df_train = preprocess_data(df_train)\n",
        "    df_test = preprocess_data(df_test)\n",
        "    df_train = sentiment_analysis(df_train)\n",
        "    df_test = sentiment_analysis(df_test)\n",
        "\n",
        "    # Vectorización del texto\n",
        "    X_train, X_test = vectorize_text(df_train, df_test)\n",
        "    y_train, y_test = df_train['polaridad'], df_test['polaridad']\n",
        "\n",
        "    return X_train, X_test, y_train, y_test, df_train, df_test\n",
        "\n",
        "# Ejecutar el pipeline y los controles de calidad\n",
        "X_train, X_test, y_train, y_test, df_train, df_test = data_pipeline()\n",
        "quality_checks(df_train)\n",
        "quality_checks(df_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SiG1oVPHvPI_",
        "outputId": "3f428412-4c5f-42ae-f8b7-ee47de6c1bb4"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Todos los controles de calidad se aprobaron.\n",
            "Todos los controles de calidad se aprobaron.\n"
          ]
        }
      ]
    }
  ]
}
